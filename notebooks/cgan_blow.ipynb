{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.makedirs(\"keras/\", exist_ok=True)\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pathlib\n",
    "from contextlib import redirect_stdout\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.core.util import event_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パリソンデータから外径/板厚情報を抽出\n",
    "def extract_input_data(lines: list):\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        values = line.split()\n",
    "        data.append([float(value) for value in values])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ls-dyna出力データから*ELEMENT_SHELL_THICKNESSの情報を抽出\n",
    "def extract_result_data(lines: list):\n",
    "    data = []\n",
    "    extract = False\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"$\"):\n",
    "            continue\n",
    "        elif line.startswith(\"*ELEMENT_SHELL_THICKNESS\"):\n",
    "            extract = True\n",
    "            continue\n",
    "        elif line.startswith(\"*\"):\n",
    "            extract = False\n",
    "\n",
    "        if extract:\n",
    "            values = line.split()\n",
    "            data.append([float(value) for value in values])\n",
    "\n",
    "    id_list_list = [sublist for index, sublist in enumerate(data) if index % 2 == 0]\n",
    "    id_list = [sublist[0] for sublist in id_list_list]\n",
    "\n",
    "    result_list_list = [sublist for index, sublist in enumerate(data) if index % 2 != 0]\n",
    "    result_list = [sum(sublist) / len(sublist) for sublist in result_list_list]\n",
    "\n",
    "    result_data_list = [[x, y] for x, y in zip(id_list, result_list)]\n",
    "\n",
    "    return result_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パリソンデータのみをフィルタリング\n",
    "def filter_data(data: list):\n",
    "    filtered_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        if 500000 <= entry[0] <= 699999:\n",
    "            filtered_data.append(entry)\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "def load(data):\n",
    "    m = tf.shape(data)[1]\n",
    "    m = m // 2\n",
    "    input_data = data[:, :m, :]\n",
    "    result_data = data[:, m:, :]\n",
    "\n",
    "    return input_data, result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畳み込み\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters,\n",
    "            size,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=initializer,\n",
    "            use_bias=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逆畳み込み\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    init = tf.random_normal_initializer(0.0, 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters,\n",
    "            size,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=init,\n",
    "            use_bias=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.05))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器関数\n",
    "def funcg():\n",
    "    inp = tf.keras.layers.Input(shape=[meshy + 6, meshx + 6, 1])\n",
    "\n",
    "    std = [\n",
    "        downsample(64, 4, apply_batchnorm=False),\n",
    "        downsample(128, 4),\n",
    "        downsample(256, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "        downsample(512, 4),\n",
    "    ]\n",
    "\n",
    "    stu = [\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4, apply_dropout=True),\n",
    "        upsample(512, 4),\n",
    "        upsample(256, 4),\n",
    "        upsample(128, 4),\n",
    "        upsample(64, 4),\n",
    "    ]\n",
    "\n",
    "    init = tf.random_normal_initializer(0.0, 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        1, 4, strides=2, padding=\"same\", kernel_initializer=init, activation=\"tanh\"\n",
    "    )\n",
    "\n",
    "    rlt = tf.keras.layers.Resizing(256, 256, interpolation=\"bilinear\")(inp)\n",
    "\n",
    "    stk = []\n",
    "    for dw in std:\n",
    "        rlt = dw(rlt)\n",
    "        stk.append(rlt)\n",
    "\n",
    "    stk = reversed(stk[:-1])\n",
    "\n",
    "    for up, sk in zip(stu, stk):\n",
    "        rlt = up(rlt)\n",
    "        rlt = tf.keras.layers.Concatenate()([rlt, sk])\n",
    "\n",
    "    rlt = last(rlt)\n",
    "    rlt = tf.keras.layers.Resizing(meshy + 6, meshx + 6, interpolation=\"bilinear\")(rlt)\n",
    "\n",
    "    return tf.keras.Model(inputs=inp, outputs=rlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器の損失関数\n",
    "def lsg(dset, gdata, rlt):\n",
    "    lsg1 = ls(tf.ones_like(dset), dset)\n",
    "    lsg2 = tf.reduce_mean(tf.abs(rlt - gdata))\n",
    "    lsgs = lsg1 + lsg2 * 100\n",
    "\n",
    "    return lsgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 識別器関数\n",
    "def funcd():\n",
    "    init = tf.random_normal_initializer(0.0, 0.02)\n",
    "    inp = tf.keras.layers.Input(shape=[meshy + 6, meshx + 6, 1])\n",
    "    out = tf.keras.layers.Input(shape=[meshy + 6, meshx + 6, 1])\n",
    "    set = tf.keras.layers.concatenate([inp, out], axis=2)\n",
    "    rlt = tf.keras.layers.Resizing(256, 512, interpolation=\"bilinear\")(set)\n",
    "    rlt = downsample(64, 4, False)(rlt)\n",
    "    rlt = downsample(128, 4)(rlt)\n",
    "    rlt = downsample(256, 4)(rlt)\n",
    "    rlt = tf.keras.layers.Resizing(32, 32, interpolation=\"bilinear\")(rlt)\n",
    "    rlt = tf.keras.layers.ZeroPadding2D()(rlt)\n",
    "    rlt = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=init, use_bias=False)(rlt)\n",
    "    rlt = tf.keras.layers.BatchNormalization()(rlt)\n",
    "    rlt = tf.keras.layers.LeakyReLU()(rlt)\n",
    "    rlt = tf.keras.layers.ZeroPadding2D()(rlt)\n",
    "    rlt = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=init)(rlt)\n",
    "\n",
    "    return tf.keras.Model(inputs=set, outputs=rlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 識別器の損失関数\n",
    "def lsd(ddata, dset):\n",
    "    lsd1 = ls(tf.ones_like(ddata), ddata)\n",
    "    lsd2 = ls(tf.zeros_like(dset), dset)\n",
    "    lsds = lsd1 + lsd2\n",
    "\n",
    "    return lsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "@tf.function\n",
    "def tr(inp, rlt, kep):\n",
    "    with tf.GradientTape() as tpg, tf.GradientTape() as tpd:\n",
    "        gdata = cgang(inp, training=True)\n",
    "        setdata = tf.keras.layers.concatenate([inp, rlt], axis=2)\n",
    "        ddata = cgand(setdata, training=True)\n",
    "        gset = tf.keras.layers.concatenate([inp, gdata], axis=2)\n",
    "        dset = cgand(gset, training=True)\n",
    "        lsgs = lsg(dset, gdata, rlt)\n",
    "        lsds = lsd(ddata, dset)\n",
    "\n",
    "    grg = tpg.gradient(lsgs, cgang.trainable_variables)\n",
    "    grd = tpd.gradient(lsds, cgand.trainable_variables)\n",
    "    opg.apply_gradients(zip(grg, cgang.trainable_variables))\n",
    "    opd.apply_gradients(zip(grd, cgand.trainable_variables))\n",
    "\n",
    "    with record.as_default():\n",
    "        tf.summary.scalar(\"loss_g\", lsgs, step=kep)\n",
    "        tf.summary.scalar(\"loss_d\", lsds, step=kep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数描画\n",
    "def lossplot(data, lpath):\n",
    "    data = np.reshape(data, (-1, 2)).transpose()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(data[0], data[1])\n",
    "    plt.xlim(-5, 100)\n",
    "    plt.ylim(0, 25)\n",
    "    plt.xlabel(\"Step\", fontsize=20)\n",
    "    plt.ylabel(\"Loss\", fontsize=20)\n",
    "    plt.tick_params(labelsize=20)\n",
    "    plt.savefig(lpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メインプログラム\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ntr = 10  # 教師データ数\n",
    "    epoch = 100  # エポック数\n",
    "    step = ntr * epoch  # ステップ数\n",
    "    meshx = 100  # x方向メッシュ数\n",
    "    meshy = 100  # y方向メッシュ数\n",
    "    meshcut = meshx // 4  # メッシュ展開位置\n",
    "    meshhalf = meshx // 2  # 2分割数\n",
    "    setdata = np.empty([0, meshy + 6, meshx * 2 + 12, 1])\n",
    "\n",
    "    for k in range(ntr):\n",
    "\n",
    "        # パリソンデータの読み込み\n",
    "        filepr = \"/tmp/traindata/parison\" + str(k + 1) + \".dat\"\n",
    "        with open(filepr, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        input_data = extract_input_data(lines)\n",
    "        del input_data[0:1]\n",
    "        input_data = np.repeat(input_data, meshhalf).reshape(meshy, meshx)\n",
    "        input_data[:, :meshhalf] = (input_data[:, :meshhalf] - 330) / 18\n",
    "        input_data[:, meshhalf:] = input_data[:, meshhalf:] - 11\n",
    "        input_data = np.pad(input_data, [(3, 3), (3, 3)], \"constant\")\n",
    "\n",
    "        # LS-dyna出力ファイルの読み込み\n",
    "        filedn = \"/tmp/traindata/lsdyna\" + str(k + 1) + \".k\"\n",
    "        with open(filedn, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        result_data_list = extract_result_data(lines)\n",
    "        filtered_result_list = filter_data(result_data_list)\n",
    "        result_data = [sublist[1] for sublist in filtered_result_list]\n",
    "        result_data = np.reshape(result_data, (meshy, meshx))\n",
    "        result_data = np.block(\n",
    "            [result_data[:, meshx - meshcut :], result_data[:, : meshx - meshcut]]\n",
    "        )\n",
    "        result_data = (result_data - 10) / 10\n",
    "        result_data = np.pad(result_data, [(3, 3), (3, 3)], \"constant\")\n",
    "        data = np.block([input_data, result_data]).reshape(\n",
    "            1, meshy + 6, meshx * 2 + 12, 1\n",
    "        )\n",
    "        setdata = np.concatenate([setdata, data], 0).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 教師データの前処理\n",
    "trdata = tf.data.Dataset.from_tensor_slices(setdata)\n",
    "trdata = trdata.map(load)\n",
    "trdata = trdata.shuffle(ntr)\n",
    "trdata = trdata.batch(1)\n",
    "\n",
    "# 生成器、識別器の呼び出し\n",
    "cgang = funcg()\n",
    "cgand = funcd()\n",
    "opg = tf.keras.optimizers.RMSprop(5e-4, rho=0.95)\n",
    "opd = tf.keras.optimizers.RMSprop(1e-5, rho=0.8)\n",
    "ls = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ログファイル書き出し\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "stdtime = now\n",
    "sttime = time.time()\n",
    "print(f'')\n",
    "print(f'---------------------------------------------------------------------------------------------')\n",
    "print(f'    Deep Learning by Conditional Generative Adversarial Networks      {now}')\n",
    "print(f'')\n",
    "print(f'       Training dataset {ntr}')\n",
    "print(f'       Epoch {epoch}')\n",
    "print(f'       Total step {step}')\n",
    "print(f'       Mesh size {meshx} X {meshy}')\n",
    "print(f'---------------------------------------------------------------------------------------------')\n",
    "print(f'')\n",
    "\n",
    "o = open(\"mes.dat\",\"w\")\n",
    "print(f'', file=o)\n",
    "print(f'---------------------------------------------------------------------------------------------', file=o)\n",
    "print(f'    Deep Learning by Conditional Generative Adversarial Networks      {now}', file=o)\n",
    "print(f'', file=o)\n",
    "print(f'       Training dataset {ntr}', file=o)\n",
    "print(f'       Epoch {epoch}', file=o)\n",
    "print(f'       Total step {step}', file=o)\n",
    "print(f'       Mesh size {meshx} X {meshy}', file=o)\n",
    "print(f'---------------------------------------------------------------------------------------------', file=o)\n",
    "print(f'', file=o)\n",
    "\n",
    "log_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "record = tf.summary.create_file_writer(\"loss_history/event_file/\" + log_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "init = time.time()\n",
    "pvt = init\n",
    "\n",
    "for kep, (inp, rlt) in trdata.repeat().take(step+1).enumerate():\n",
    "    tr(inp, rlt, kep)\n",
    "\n",
    "if kep != 0 and (kep) % 10 == 0:\n",
    "    crt = time.time()\n",
    "    now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'  Step {kep}   dt {crt-pvt:.2f} sec   total time {crt-init:.2f} sec       {now}')\n",
    "    print(f'  Step {kep}   dt {crt-pvt:.2f} sec   total time {crt-init:.2f} sec       {now}', file=o)\n",
    "    pvt = time.time()\n",
    "\n",
    "if kep == step:\n",
    "    print(f'')\n",
    "    print(f'', file=o)\n",
    "    print(f'       Step end reached')\n",
    "    print(f'       Step end reached', file=o)\n",
    "    print(f'')\n",
    "    print(f'')\n",
    "    print(f'', file=o)\n",
    "    print(f'', file=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数プロット\n",
    "ev = \"loss_history/event_file/\" + log_time\n",
    "\n",
    "og = open(\"./loss_history/cgan_loss_g.dat\",\"w\")\n",
    "od = open(\"./loss_history/cgan_loss_d.dat\",\"w\")\n",
    "print(f'step  loss_g', file=og)\n",
    "print(f'step  loss_d', file=od)\n",
    "lgpath = \"./loss_history/cgan_loss_g.png\"\n",
    "ldpath = \"./loss_history/cgan_loss_d.png\"\n",
    "\n",
    "datag = []\n",
    "datad = []\n",
    "\n",
    "for fname in os.listdir(ev):\n",
    "    path = os.path.join(ev, fname)\n",
    "    ldataset = tf.data.TFRecordDataset(path)\n",
    "\n",
    "    for ldata in ldataset:\n",
    "       event = event_pb2.Event.FromString(ldata.numpy())\n",
    "       for value in event.summary.value:\n",
    "            t = tf.make_ndarray(value.tensor)\n",
    "\n",
    "            if value.tag == 'loss_g':\n",
    "                print(event.step, t, file=og)\n",
    "                datag = np.append(datag, event.step)\n",
    "                datag = np.append(datag, t)\n",
    "\n",
    "            if value.tag == 'loss_d':\n",
    "                print(event.step, t, file=od)\n",
    "                datad = np.append(datad, event.step)\n",
    "                datad = np.append(datad, t)\n",
    "\n",
    "lossplot(datag,lgpath)\n",
    "lossplot(datad,ldpath)\n",
    "\n",
    "og.close()\n",
    "od.close()\n",
    "\n",
    "# 出力\n",
    "print(f'---------------------------------------------------------------------------------------------')\n",
    "print(f'    Convolutional Neural Network Summary')\n",
    "print(f'---------------------------------------------------------------------------------------------')\n",
    "print(f'')\n",
    "\n",
    "print(f'---------------------------------------------------------------------------------------------', file=o)\n",
    "print(f'    Convolutional Neural Network Summary', file=o)\n",
    "print(f'---------------------------------------------------------------------------------------------', file=o)\n",
    "print(f'', file=o)\n",
    "\n",
    "cgang.summary()\n",
    "with redirect_stdout(o):\n",
    "    cgang.summary()\n",
    "\n",
    "print(f'')\n",
    "print(f'', file=o)\n",
    "\n",
    "cgand.summary()\n",
    "with redirect_stdout(o):\n",
    "    cgand.summary()\n",
    "\n",
    "cgang.save('./keras/cgang.keras')\n",
    "cgand.save('./keras/cgand.keras')\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "edtime = time.time()\n",
    "eptime = edtime - sttime\n",
    "hour = int(eptime / 3600)\n",
    "minute = int((eptime / 60) % 60)\n",
    "second = int(eptime % 60)\n",
    "\n",
    "print(f'')\n",
    "print(f'')\n",
    "print(f'  Start time    {stdtime}')\n",
    "print(f'  End time      {now}')\n",
    "print(f'  Elapsed time     {eptime:.2f} seconds  ( {hour} hour  {minute} minutes  {second} seconds )')\n",
    "print(f'')\n",
    "print(f'---- Normal Termination ----                       {now}')\n",
    "print(f'')\n",
    "\n",
    "print(f'', file=o)\n",
    "print(f'', file=o)\n",
    "print(f'  Start time    {stdtime}', file=o)\n",
    "print(f'  End time      {now}', file=o)\n",
    "print(f'  Elapsed time     {eptime:.2f} seconds  ( {hour} hour  {minute} minutes  {second} seconds )', file=o)\n",
    "print(f'', file=o)\n",
    "print(f'---- Normal Termination ----                       {now}', file=o)\n",
    "print(f'', file=o)\n",
    "\n",
    "o.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
